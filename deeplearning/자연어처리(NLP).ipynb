{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c6f0da",
   "metadata": {},
   "source": [
    "## tf.keras.preprocessing.text.text_to_word_sequence\n",
    "\n",
    "* tf.keras.preprocessing.text.text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' '\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "* Args\n",
    "input_text\tInput text (string).\n",
    "filters\tlist (or concatenation) of characters to filter out, such as punctuation. Default: '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n', includes basic punctuation, tabs, and newlines.\n",
    "lower\tboolean. Whether to convert the input to lowercase.\n",
    "split\tstr. Separator for word splitting.\n",
    "Returns\n",
    "A list of words (or tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47d1a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:03:22.793721Z",
     "start_time": "2023-02-28T02:03:22.787798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['해보지', '않으면', '해낼', '수', '없다']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화하기 (입력된 텍스트를 공백에 맞춰 잘게 나누기)\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text ='해보지 않으면, 해낼 수 없다'\n",
    "\n",
    "text_to_word_sequence(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2893295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:03:23.502074Z",
     "start_time": "2023-02-28T02:03:23.496118Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['해보지 않으면', ' 해낼 수 없다']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split= 을 지정해서 텍스트를 나누는 기준을 설정할 수 있다\n",
    "text_to_word_sequence(text, split='  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12d7e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:03:24.223711Z",
     "start_time": "2023-02-28T02:03:24.217104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['해보지 않으면', ' 해낼 수 없다']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(text, split=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7446f",
   "metadata": {},
   "source": [
    "문장을 토큰화하면 이를 이용해 여러가지를 할 수 있음\n",
    "- 단어 빈도 확인\n",
    "- 중요 단어 파악\n",
    "\n",
    "Bag-of-words 라는 방법이 이런 전처리를 일컫는 말 / 같은 단어끼리 따로 따로 가방에 담아 각 가방에 몇 개의 단어가 있는지 세는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa444a",
   "metadata": {},
   "source": [
    "### tf.keras.preprocessing.text.Tokenizer\n",
    "\n",
    "* tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=' ',\n",
    "    char_level=False,\n",
    "    oov_token=None,\n",
    "    analyzer=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "\n",
    "* Args\n",
    "num_words\tthe maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
    "filters\ta string where each element is a character that will be filtered from the texts. The default is all punctuation, plus tabs and line breaks, minus the ' character.\n",
    "lower\tboolean. Whether to convert the texts to lowercase.\n",
    "split\tstr. Separator for word splitting.\n",
    "char_level\tif True, every character will be treated as a token.\n",
    "oov_token\tif given, it will be added to word_index and used to replace out-of-vocabulary words during text_to_sequence calls\n",
    "analyzer\tfunction. Custom analyzer to split the text. The default analyzer is text_to_word_sequence\n",
    "\n",
    "\n",
    "* ## Methods\n",
    "\n",
    "### `fit_on_sequences`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L329-L343)\n",
    "\n",
    "```\n",
    "fit_on_sequences(    sequences)\n",
    "```\n",
    "\n",
    "Updates internal vocabulary based on a list of sequences.\n",
    "\n",
    "Required before using  `sequences_to_matrix`  (if  `fit_on_texts`  was never called).\n",
    "\n",
    "Args\n",
    "\n",
    "`sequences`\n",
    "\n",
    "A list of sequence. A \"sequence\" is a list of integer word indices.\n",
    "\n",
    "### `fit_on_texts`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L269-L327)\n",
    "\n",
    "```\n",
    "fit_on_texts(    texts)\n",
    "```\n",
    "\n",
    "Updates internal vocabulary based on a list of texts.\n",
    "\n",
    "In the case where texts contains lists, we assume each entry of the lists to be a token.\n",
    "\n",
    "Required before using  `texts_to_sequences`  or  `texts_to_matrix`.\n",
    "\n",
    "Args\n",
    "\n",
    "`texts`\n",
    "\n",
    "can be a list of strings, a generator of strings (for memory-efficiency), or a list of list of strings.\n",
    "\n",
    "### `get_config`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L526-L555)\n",
    "\n",
    "```\n",
    "get_config()\n",
    "```\n",
    "\n",
    "Returns the tokenizer configuration as Python dictionary.\n",
    "\n",
    "The word count dictionaries used by the tokenizer get serialized into plain JSON, so that the configuration can be read by other projects.\n",
    "\n",
    "Returns\n",
    "\n",
    "A Python dictionary with the tokenizer configuration.\n",
    "\n",
    "### `sequences_to_matrix`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L466-L524)\n",
    "\n",
    "```\n",
    "sequences_to_matrix(    sequences, mode='binary')\n",
    "```\n",
    "\n",
    "Converts a list of sequences into a Numpy matrix.\n",
    "\n",
    "Args\n",
    "\n",
    "`sequences`\n",
    "\n",
    "list of sequences (a sequence is a list of integer word indices).\n",
    "\n",
    "`mode`\n",
    "\n",
    "one of \"binary\", \"count\", \"tfidf\", \"freq\"\n",
    "\n",
    "Returns\n",
    "\n",
    "A Numpy matrix.\n",
    "\n",
    "Raises\n",
    "\n",
    "`ValueError`\n",
    "\n",
    "In case of invalid  `mode`  argument, or if the Tokenizer requires to be fit to sample data.\n",
    "\n",
    "### `sequences_to_texts`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L407-L419)\n",
    "\n",
    "```\n",
    "sequences_to_texts(    sequences)\n",
    "```\n",
    "\n",
    "Transforms each sequence into a list of text.\n",
    "\n",
    "Only top  `num_words-1`  most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.\n",
    "\n",
    "Args\n",
    "\n",
    "`sequences`\n",
    "\n",
    "A list of sequences (list of integers).\n",
    "\n",
    "Returns\n",
    "\n",
    "A list of texts (strings)\n",
    "\n",
    "### `sequences_to_texts_generator`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L421-L451)\n",
    "\n",
    "```\n",
    "sequences_to_texts_generator(    sequences)\n",
    "```\n",
    "\n",
    "Transforms each sequence in  `sequences`  to a list of texts(strings).\n",
    "\n",
    "Each sequence has to a list of integers. In other words, sequences should be a list of sequences\n",
    "\n",
    "Only top  `num_words-1`  most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.\n",
    "\n",
    "Args\n",
    "\n",
    "`sequences`\n",
    "\n",
    "A list of sequences.\n",
    "\n",
    "Yields\n",
    "\n",
    "Yields individual texts.\n",
    "\n",
    "### `texts_to_matrix`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L453-L464)\n",
    "\n",
    "```\n",
    "texts_to_matrix(    texts, mode='binary')\n",
    "```\n",
    "\n",
    "Convert a list of texts to a Numpy matrix.\n",
    "\n",
    "Args\n",
    "\n",
    "`texts`\n",
    "\n",
    "list of strings.\n",
    "\n",
    "`mode`\n",
    "\n",
    "one of \"binary\", \"count\", \"tfidf\", \"freq\".\n",
    "\n",
    "Returns\n",
    "\n",
    "A Numpy matrix.\n",
    "\n",
    "### `texts_to_sequences`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L345-L357)\n",
    "\n",
    "```\n",
    "texts_to_sequences(    texts)\n",
    "```\n",
    "\n",
    "Transforms each text in texts to a sequence of integers.\n",
    "\n",
    "Only top  `num_words-1`  most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.\n",
    "\n",
    "Args\n",
    "\n",
    "`texts`\n",
    "\n",
    "A list of texts (strings).\n",
    "\n",
    "Returns\n",
    "\n",
    "A list of sequences.\n",
    "\n",
    "### `texts_to_sequences_generator`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L359-L405)\n",
    "\n",
    "```\n",
    "texts_to_sequences_generator(    texts)\n",
    "```\n",
    "\n",
    "Transforms each text in  `texts`  to a sequence of integers.\n",
    "\n",
    "Each item in texts can also be a list, in which case we assume each item of that list to be a token.\n",
    "\n",
    "Only top  `num_words-1`  most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.\n",
    "\n",
    "Args\n",
    "\n",
    "`texts`\n",
    "\n",
    "A list of texts (strings).\n",
    "\n",
    "Yields\n",
    "\n",
    "Yields individual sequences.\n",
    "\n",
    "### `to_json`\n",
    "\n",
    "[View source](https://github.com/keras-team/keras/tree/v2.11.0/keras/preprocessing/text.py#L557-L575)\n",
    "\n",
    "```\n",
    "to_json(    **kwargs)\n",
    "```\n",
    "\n",
    "Returns a JSON string containing the tokenizer configuration.\n",
    "\n",
    "To load a tokenizer from a JSON string, use  [`keras.preprocessing.text.tokenizer_from_json(json_string)`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/tokenizer_from_json).\n",
    "\n",
    "Args\n",
    "\n",
    "`**kwargs`\n",
    "\n",
    "Additional keyword arguments to be passed to  `json.dumps()`.\n",
    "\n",
    "Returns\n",
    "\n",
    "A JSON string containing the tokenizer configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0725152",
   "metadata": {},
   "source": [
    "## padding - pad_sequences()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3796fb",
   "metadata": {},
   "source": [
    "## Embedding() - text_to_sequence (원핫인코딩) 대신 사용 -> 이후 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ee7c38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T04:19:04.512437Z",
     "start_time": "2023-02-28T04:19:04.504586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "원문:\n",
      " 해보지 않으면 해낼 수 없다\n",
      "\n",
      "토큰화:\n",
      " ['해보지', '않으면', '해낼', '수', '없다']\n",
      "\n",
      "단어 카운트:\n",
      " OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화', 3), ('합니다', 1), ('단어로', 1), ('해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('한', 1), ('결과는', 1), ('사용', 1), ('할', 1), ('수', 1), ('있습니다', 1)])\n",
      "\n",
      "문장 카운트: 3\n",
      "\n",
      "각 단어가 몇개의 문장에 포함되어 있는가:\n",
      " defaultdict(<class 'int'>, {'합니다': 1, '텍스트의': 2, '나누어': 1, '단어를': 1, '각': 1, '토큰화': 3, '먼저': 1, '딥러닝에서': 2, '인식됩니다': 1, '해야': 1, '단어로': 1, '할': 1, '결과는': 1, '한': 1, '있습니다': 1, '사용': 1, '수': 1})\n",
      "\n",
      "각 단어에 매겨진 인덱스 값:\n",
      " {'토큰화': 1, '텍스트의': 2, '딥러닝에서': 3, '먼저': 4, '각': 5, '단어를': 6, '나누어': 7, '합니다': 8, '단어로': 9, '해야': 10, '인식됩니다': 11, '한': 12, '결과는': 13, '사용': 14, '할': 15, '수': 16, '있습니다': 17}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "\n",
    "# 주어진 문장을 '단어'로 토큰화 하기\n",
    "# 케라스의 텍스트 전처리와 관련한 함수 중 test_to_word_sequence 함수를 불러 오기\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# 전처리할 텍스트를 정합니다.\n",
    "text = '해보지 않으면 해낼 수 없다'\n",
    "\n",
    "# 해당 텍스트를 토큰화 합니다.\n",
    "result = text_to_word_sequence(text)\n",
    "print('\\n원문:\\n', text)\n",
    "print('\\n토큰화:\\n', result)\n",
    "\n",
    "# 단어 빈도수 세기\n",
    "# 전처리 하려는 세 개의 문장을 정함\n",
    "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화 합니다.',\n",
    "       '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다.',\n",
    "       '토큰화 한 결과는 딥러닝에서 사용 할 수 있습니다.',\n",
    "       ]\n",
    "\n",
    "# 토큰화 함수를 이용해 전처리 하는 과정입니다.\n",
    "token = Tokenizer()      # 토큰화 함수 지정\n",
    "token.fit_on_texts(docs) # 토큰화 함수에 문장 적용\n",
    "\n",
    "# 단어의 빈도수를 계산한 결과를 각 옵션에 맞추어 출력\n",
    "print('\\n단어 카운트:\\n', token.word_counts)\n",
    "# Tokenizer()의 word_counts 함수는 순서를 기억하는 OrderedDict 클래스를 사용\n",
    "\n",
    "# 출력되는 순서는 랜덤\n",
    "print('\\n문장 카운트:', token.document_count)\n",
    "print('\\n각 단어가 몇개의 문장에 포함되어 있는가:\\n', token.word_docs)\n",
    "print('\\n각 단어에 매겨진 인덱스 값:\\n', token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb33bde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T04:27:20.075239Z",
     "start_time": "2023-02-28T04:27:18.746219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'너무': 1, '재밌네요': 2, '최고에요': 3, '참': 4, '잘': 5, '만든': 6, '영화에요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한번': 11, '더': 12, '보고싶네요': 13, '글쎄요': 14, '별로에요': 15, '생각보다': 16, '지루하네요': 17, '연기가': 18, '어색해요': 19, '재미없어요': 20}\n",
      "\n",
      "리뷰 텍스트, 토큰화 결과:\n",
      " [[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13], [14], [15], [16, 17], [18, 19], [20]]\n",
      "\n",
      "패딩 결과:\n",
      " [[ 0  0  1  2]\n",
      " [ 0  0  0  3]\n",
      " [ 4  5  6  7]\n",
      " [ 0  8  9 10]\n",
      " [ 0 11 12 13]\n",
      " [ 0  0  0 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0 16 17]\n",
      " [ 0  0 18 19]\n",
      " [ 0  0  0 20]]\n",
      "\n",
      "딥러닝 모델 시작:\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 600ms/step - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6918 - accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6897 - accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6855 - accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6834 - accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6814 - accuracy: 0.6000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6793 - accuracy: 0.6000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6772 - accuracy: 0.6000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6752 - accuracy: 0.6000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6731 - accuracy: 0.7000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6710 - accuracy: 0.8000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6690 - accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6669 - accuracy: 0.8000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6649 - accuracy: 0.8000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6628 - accuracy: 0.8000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6607 - accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6587 - accuracy: 0.9000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6566 - accuracy: 0.9000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.9000\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.6524 - accuracy: 0.9000\n",
      "\n",
      " Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 리뷰 10개 샘플을 지정\n",
    "docs = ['너무 재밌네요', '최고에요', '참 잘 만든 영화에요',\n",
    "       '추천하고 싶은 영화입니다.', '한번 더 보고싶네요',\n",
    "       '글쎄요', '별로에요', '생각보다 지루하네요', '연기가 어색해요', '재미없어요']\n",
    "\n",
    "# 긍정 리뷰는 1, 부정 리뷰는 0으로 클래스 지정\n",
    "classes = array([1,1,1,1,1,0,0,0,0,0])\n",
    "\n",
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)\n",
    "x = token.texts_to_sequences(docs)\n",
    "print('\\n리뷰 텍스트, 토큰화 결과:\\n', x)\n",
    "\n",
    "# 패딩, 서로 다른 길이의 데이터를 4로 맞춤\n",
    "padded_x = pad_sequences(x, 4)\n",
    "print('\\n패딩 결과:\\n', padded_x)\n",
    "\n",
    "# 딥러닝 모델\n",
    "print('\\n딥러닝 모델 시작:')\n",
    "\n",
    "# 임베딩에 입력될 단어의 수를 지정\n",
    "word_size = len(token.word_index) +1\n",
    "\n",
    "# 단어 임베딩을 포함하여 딥러닝 모델을 만들고 결과 출력\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(padded_x, classes, epochs=20)\n",
    "print('\\n Accuracy: %.4f' % (model.evaluate(padded_x, classes)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f6e162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
